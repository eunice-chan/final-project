# Preliminary Prospectus
Eunice Chan

For the Data Science Honors project, I intend to tackle the problem of video generation for animated videos. Typically, video generation models have been trained on and applied to natural videos, such as those shot on a smartphone. Animated videos, especially 2D hand-drawn videos, have the unique property: the subjects are not bound to the laws of physics and can contort and change drastically in appearance. In short, animation has much more creative license. Thus, this is a harder problem than video generation of natural videos. Furthermore, many insights from exploring this space will be easily transferred to the problem of video generation, since they are so closely related. Because of the aforementioned reasons, I believe this problem is worth being tackled.

I plan to approach this project in four stages. In the first stage, I will research the current work in the field of generative models with a time component with a particular focus on video, which is directly related to the project, and speech or music, which is not as directly related, but shares similar properties which may transfer over. After that, in the second stage, I will choose the video generation models that seem the most promising and extensible. In the third stage, I will apply the models directly to a selection of animated videos. Then, I will compare the performance of the various models and evaluate how well each does, whether it performs better with certain tasks than others, and potentially dig into the "why". Finally, in the fourth step, I will either create or take a pre-existing model and apply my findings to it to create a model to do tweening for 2D animations. Then, I will evaluate the performance, and compare the results with different types of videos: 2D, 3D, natural videos, etc. Realistically, I believe that I can reach the third stage by the end of the semester depending on how broadly I want to cast my net. Ideally, I would be able to reach the fourth stage and iterate over the project.

The data I plan to use is animated films that have passed to the public domain. If the dataset does not seem sufficiently large, I plan to supplement the data with public domain video and movies, preferably ones with a lot of special effects. I should be able to supplement with natural video and movies because they can be argued to be a specific type of animated video. With this dataset, a more realistic style of animation is represented and that might bias the model to perform better with that style of animation. Thus, I may need to spend some time improving the dataset to include a greater variety of videos and animation styles.

Some exploratory data analysis I would like to do on the dataset before starting on the project is to extract potentially helpful features from each clip and seeing the distribution of the features in the set to get an idea of the biases. This step can overlap with the data augmentation step. For data augmentation, I plan to focus on modifying the style of the videos so that the model will not overfit to the dataset. Some potential techniques I would apply are adjusting contrast, applying edge detection algorithms, as well as changing colors, if I decide to keep that attribute. Another thing that may be interesting is applying StyleGAN to modify the style of the images to vastly increase the size of the dataset. What techniques I will use will depend primarily on what I read in the papers. Based on my existing knowledge, I will primarily focus on GANs (generative adversarial networks). The papers I will focus on are those concerning natural video generation models, deepfake generation, and coherent text generation using language generation models. Because of this, I want to work with someone with experience with GAN. Ideally, the person would also be familiar to computer graphics, specifically animation.

This links with my interests as I am primarily interested in machine learning and graphics, primarily animation. In my digital humanities minor, there was a focus on natural language processing of textual data and in a past research project I was involved in led by a PhD student in Linguistics, we worked on classification of annotated speech data with a LTSM model. Although I focused primarily on linguistics data, because I am dealing with data through time, a  lot of what I learned could transfer over, as well as give me a less common perspective when approaching the problem. I also did kinematics (inverse and forward) for a 3D figure for my final project in computer graphics. Inverse kinematics explores finding parameters to pose a model such that a specified point matches a target pose (that is, a specified rotation, position, and so forth). This problem is similar to the problem I am interested in exploring in this project: both have an initial state, both have a target state, and both are trying to interpolate between. I am interested in seeing if I can apply concepts and approaches from inverse kinematics to video generation and how well the results will turn out.

To provide context behind this project, I am generally interested in automating the boring stuff. With the prevalence of deepfakes (to the point making deepfakes of talking heads to sing Baka Mitai is a meme), I believe the video generation research is bountiful enough that I will be able to apply this to animation and explore speeding up the animation process with such research. Animation can be tedious, especially filling in the frame in-between (the inbetweens, in the process called tweening) the frames signifying the key movements (the keyframes). Many "animations'' on Youtube, be in animatic artists, or storytelling artists, tend to be very limited in movement or just draw the keyframes and let the viewer fill in the rest. In fact, traditional 2D animation is made by an experienced animator drawing the keyframes and letting less experienced animators fill in the inbetweens. All of this has me convinced that being able to automatically generate inbetweens from keyframes is not just feasible, but valuable to, and could be adopted by many people. This however, also poses a question about the human context of the research and the ethics as it may be used to replace some people's jobs. I personally believe it just removes the grunt work and allows people to focus on the content, but I am also concerned about the potential impact. The images produced would very likely contain a lot of artifacts, so another possibility could be that the nature of the work done by junior animators would change to be so that they are cleaning up an existing piece of work. I want to make it so that the consequences of what I do is more good than bad, or even neutral and for that, I would like to make the implementation as accessible as possible so that normal people, not just companies, will be able to use this to produce visually appealing and engaging content without needing to fuss too much about the technical details, thus lowering the barrier to creating high quality work and allowing a greater variety of content to reach a wider audience.
